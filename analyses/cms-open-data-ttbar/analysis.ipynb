{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "831639dc-6247-48e3-98e9-4708e7629896",
   "metadata": {},
   "source": [
    "# CMS Open Data $t\\bar{t}$ with ROOT RDataFrame\n",
    "\n",
    "This notebook demonstrates a physics analysis of the $t\\bar{t}$ cross-section measurement with [2015 CMS Open Data](https://cms.cern/news/first-cms-open-data-lhc-run-2-released).\n",
    "\n",
    "It is developed using ROOT RDataFrame as the analysis interface. It implements the equivalent version of the [v1.0.0](https://agc.readthedocs.io/en/latest/versionsdescription.html) of the reference implementation, including showing how to specify the input dataset, the various steps involved in processing data and the output histograms. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b5f03b1-6f8b-4b22-8332-77898d108b34",
   "metadata": {},
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82bffc-6258-4fee-b7e7-34e2be4c0ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from typing import Optional\n",
    "\n",
    "import ROOT\n",
    "from distributed import Client, LocalCluster, SSHCluster, get_worker\n",
    "from plotting import save_plots\n",
    "from utils import (\n",
    "    AGCInput,\n",
    "    AGCResult,\n",
    "    postprocess_results,\n",
    "    retrieve_inputs,\n",
    "    save_histos,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61eb3e40-e631-43af-bcc4-cac263c5b2c2",
   "metadata": {},
   "source": [
    "## Analysis configuration\n",
    "\n",
    "The next cell contains the parameters that can be tuned to configure various aspects of the notebook execution. Changing the default value of any variable in the cell modifies the corresponding option. The options are described in the following table:\n",
    "\n",
    "| Setting                | Meaning                                                                                                                                                                                                                                                                                                 |\n",
    "|------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| DATA_CACHE             | Use the specified directory as a local data cache: required input datasets will be downloaded here and the analysis will read this local copy of the data.                                                                                                                                              |\n",
    "| HOSTS                  | A comma-separated list of worker node hostnames. Only required if --scheduler=dask-ssh, ignored otherwise.                                                                                                                                                                                              |\n",
    "| NCORES                 | Number of cores to use. In case of distributed execution this is the amount of cores per node.                                                                                                                                                                                                          |\n",
    "| N_MAX_FILES_PER_SAMPLE | How many files per sample will be processed. If left unchanged, all files for all samples                                                                                                                                                                                                                       |\n",
    "| NPARTITIONS            | Number of data partitions. Only used in case of distributed execution. By default it follows RDataFrame defaults.                                                                                                                                                                                       |\n",
    "| OUTPUT                 | Name of the file where analysis results will be stored. If it already exists, contents are overwritten.                                                                                                                                                                                                 |\n",
    "| REMOTE_DATA_PREFIX     | The original data is stored at 'https://xrootd-local.unl.edu:1094//store/user/AGC'. Changing the prefix is replaced with the argument to this option when accessing remote data. For example for the version of the input datasets stored on EOS use `'root://eoscms.cern.ch//eos/cms/store/test/agc'`. |\n",
    "| SCHEDULER              | The scheduler for RDataFrame parallelization. Will honor the --ncores flag. The default is `mt`, i.e. multi-thread execution. If `dask-ssh`, a list of worker node hostnames to connect to should be provided via the --nodes option.                                                                   |\n",
    "| VERBOSE                | Turn on verbose execution logs.                                                                                                                                                                                                                                                                         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b48546c-0301-45a2-a397-15fd00aeb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CACHE: Optional[str] = None\n",
    "\n",
    "HOSTS: Optional[str] = None\n",
    "\n",
    "NCORES: int = len(os.sched_getaffinity(0))\n",
    "\n",
    "N_MAX_FILES_PER_SAMPLE: Optional[int] = None\n",
    "\n",
    "NPARTITIONS: Optional[int] = None\n",
    "\n",
    "OUTPUT: str = \"histograms.root\"\n",
    "\n",
    "REMOTE_DATA_PREFIX: Optional[str] = None\n",
    "\n",
    "SCHEDULER: str = \"mt\"\n",
    "\n",
    "VERBOSE: bool = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6947e4c-9dcf-4782-aa63-0d48a6b74f8c",
   "metadata": {},
   "source": [
    "## Configuration of cluster connection via Dask\n",
    "\n",
    "This notebook can be executed both on the local machine but also seamlessly distributed over multiple nodes via Dask. The next utility function creates a Dask `Client` object based on the `SCHEDULER` option chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385105bd-0574-43ba-a479-f0d8df98dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dask_client(scheduler: str, ncores: int, hosts: str) -> Client:\n",
    "    \"\"\"Create a Dask distributed client.\"\"\"\n",
    "    if scheduler == \"dask-local\":\n",
    "        lc = LocalCluster(n_workers=ncores, threads_per_worker=1, processes=True)\n",
    "        return Client(lc)\n",
    "\n",
    "    if scheduler == \"dask-ssh\":\n",
    "        workers = hosts.split(\",\")\n",
    "        print(f\"Using worker nodes: {workers=}\")\n",
    "        # The creation of the SSHCluster object might need to be further configured to fit specific use cases.\n",
    "        # For example, in some clusters the \"local_directory\" key must be supplied in the worker_options dictionary.\n",
    "        sshc = SSHCluster(\n",
    "            workers,\n",
    "            connect_options={\"known_hosts\": None},\n",
    "            worker_options={\"nprocs\": ncores, \"nthreads\": 1, \"memory_limit\": \"32GB\"},\n",
    "        )\n",
    "        return Client(sshc)\n",
    "\n",
    "    raise ValueError(\n",
    "        f\"Unexpected scheduling mode '{scheduler}'. Valid modes are ['dask-local', 'dask-ssh'].\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4925e148-69eb-4ad3-a529-090dbf8bd0b7",
   "metadata": {},
   "source": [
    "## Loading C++ helper functions\n",
    "\n",
    "The notebook will use a shared library obtained by compiling a C++ source file with optimizations before starting the analysis. The next helper function does exactly that, using the correct path to the C++ file: either from the same directory of the notebook or from the path on the distributed worker node filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b345a-76eb-493f-ada7-536245359534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cpp():\n",
    "    \"\"\"Load C++ helper functions. Works for both local and distributed execution.\"\"\"\n",
    "    try:\n",
    "        # when using distributed RDataFrame 'helpers.cpp' is copied to the local_directory\n",
    "        # of every worker (via `distribute_unique_paths`)\n",
    "        localdir = get_worker().local_directory\n",
    "        cpp_source = Path(localdir) / \"helpers.cpp\"\n",
    "    except ValueError:\n",
    "        # must be local execution\n",
    "        cpp_source = \"helpers.cpp\"\n",
    "\n",
    "    ROOT.gSystem.CompileMacro(str(cpp_source), \"kO\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b7b6406-87f2-4652-81ce-4028327820dc",
   "metadata": {},
   "source": [
    "# Defining the analysis steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a5475-e2c6-43d6-beed-2c0388a05d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using https://atlas-groupdata.web.cern.ch/atlas-groupdata/dev/AnalysisTop/TopDataPreparation/XSection-MC15-13TeV.data\n",
    "# as a reference. Values are in pb.\n",
    "XSEC_INFO = {\n",
    "    \"ttbar\": 396.87 + 332.97,  # nonallhad + allhad, keep same x-sec for all\n",
    "    \"single_top_s_chan\": 2.0268 + 1.2676,\n",
    "    \"single_top_t_chan\": (36.993 + 22.175) / 0.252,  # scale from lepton filter to inclusive\n",
    "    \"single_top_tW\": 37.936 + 37.906,\n",
    "    \"wjets\": 61457 * 0.252,  # e/mu+nu final states\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded05b3-ef64-4cf8-80f5-ee25c78af055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rdf(\n",
    "    files: list[str], client: Optional[Client], npartitions: Optional[int]\n",
    ") -> ROOT.RDataFrame:\n",
    "    \"\"\"Construct and return a dataframe or, if a dask client is present, a distributed dataframe.\"\"\"\n",
    "    if client is not None:\n",
    "        d = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame(\n",
    "            \"Events\", files, daskclient=client, npartitions=npartitions\n",
    "        )\n",
    "        # Make sure the C++ helpers are uploaded to the distributed workers\n",
    "        d._headnode.backend.distribute_unique_paths(\n",
    "            [\n",
    "                \"helpers.cpp\",\n",
    "            ]\n",
    "        )\n",
    "        return d\n",
    "\n",
    "    return ROOT.RDataFrame(\"Events\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dca2e0-3e42-46c6-aea1-5e795630b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_trijet_mass(df: ROOT.RDataFrame) -> ROOT.RDataFrame:\n",
    "    \"\"\"Add the trijet_mass observable to the dataframe after applying the appropriate selections.\"\"\"\n",
    "\n",
    "    # First, select events with at least 2 b-tagged jets\n",
    "    df = df.Filter(\"Sum(Jet_btagCSVV2_ptcut > 0.5) > 1\")\n",
    "\n",
    "    # Build four-momentum vectors for each jet\n",
    "    df = (\n",
    "        df.Define(\n",
    "            \"Jet_p4\",\n",
    "            \"\"\"\n",
    "        ROOT::VecOps::Construct<ROOT::Math::PxPyPzMVector>(\n",
    "            ROOT::VecOps::Construct<ROOT::Math::PtEtaPhiMVector>(\n",
    "                Jet_pt_ptcut, Jet_eta[Jet_pt_mask], Jet_phi[Jet_pt_mask], Jet_mass[Jet_pt_mask]\n",
    "            )\n",
    "        )\n",
    "        \"\"\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Build trijet combinations\n",
    "    df = df.Define(\"Trijet_idx\", \"Combinations(Jet_pt_ptcut, 3)\")\n",
    "\n",
    "    # Trijet_btag is a helpful array mask indicating whether or not the maximum btag value in Trijet is larger than the 0.5 threshold\n",
    "    df = df.Define(\n",
    "        \"Trijet_btag\",\n",
    "        \"\"\"\n",
    "            auto J1_btagCSVV2 = Take(Jet_btagCSVV2_ptcut, Trijet_idx[0]);\n",
    "            auto J2_btagCSVV2 = Take(Jet_btagCSVV2_ptcut, Trijet_idx[1]);\n",
    "            auto J3_btagCSVV2 = Take(Jet_btagCSVV2_ptcut, Trijet_idx[2]);\n",
    "            return J1_btagCSVV2 > 0.5 || J2_btagCSVV2 > 0.5 || J3_btagCSVV2 > 0.5;\n",
    "            \"\"\"\n",
    "    )\n",
    "\n",
    "    # Assign four-momentums to each trijet combination\n",
    "    df = df.Define(\n",
    "        'Trijet_p4',\n",
    "        '''\n",
    "        auto J1 = Take(Jet_p4, Trijet_idx[0]);\n",
    "        auto J2 = Take(Jet_p4, Trijet_idx[1]);\n",
    "        auto J3 = Take(Jet_p4, Trijet_idx[2]);\n",
    "        return (J1+J2+J3)[Trijet_btag];\n",
    "        '''\n",
    "    )\n",
    "\n",
    "    # Get trijet transverse momentum values from four-momentum vectors\n",
    "    df = df.Define(\n",
    "        \"Trijet_pt\",\n",
    "        \"return Map(Trijet_p4, [](const ROOT::Math::PxPyPzMVector &v) { return v.Pt(); })\",\n",
    "    )\n",
    "\n",
    "    # Evaluate mass of trijet with maximum pt and btag higher than threshold\n",
    "    df = df.Define(\n",
    "        \"Trijet_mass\", \"Trijet_p4[ArgMax(Trijet_pt)].M()\"\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b7ba4-4349-41aa-b9a7-80ad88b0271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_histos(\n",
    "    df: ROOT.RDataFrame,\n",
    "    process: str,\n",
    "    variation: str,\n",
    "    nevents: int,\n",
    ") -> list[AGCResult]:\n",
    "    \"\"\"Return the RDataFrame results pertaining to the desired process and variation.\"\"\"\n",
    "    # Calculate normalization for MC\n",
    "    x_sec = XSEC_INFO[process]\n",
    "    lumi = 3378  # /pb\n",
    "    xsec_weight = x_sec * lumi / nevents\n",
    "    df = df.Define(\"Weights\", str(xsec_weight))  # default weights\n",
    "\n",
    "    if variation == \"nominal\":\n",
    "        # Jet_pt variations definition\n",
    "        # pt_scale_up() and pt_res_up(jet_pt) return scaling factors applying to jet_pt\n",
    "        # pt_scale_up() - jet energy scaly systematic\n",
    "        # pt_res_up(jet_pt) - jet resolution systematic\n",
    "        df = df.Vary(\n",
    "            \"Jet_pt\",\n",
    "            \"ROOT::RVec<ROOT::RVecF>{Jet_pt*pt_scale_up(), Jet_pt*jet_pt_resolution(Jet_pt.size())}\",\n",
    "            [\"pt_scale_up\", \"pt_res_up\"],\n",
    "        )\n",
    "\n",
    "        if process == \"wjets\":\n",
    "            # Flat weight variation definition\n",
    "            df = df.Vary(\n",
    "                \"Weights\",\n",
    "                \"Weights*flat_variation()\",\n",
    "                [f\"scale_var_{direction}\" for direction in [\"up\", \"down\"]],\n",
    "            )\n",
    "\n",
    "    # Event selection - the core part of the algorithm applied for both regions\n",
    "    # Selecting events containing at least one lepton and four jets with pT > 25 GeV\n",
    "    # Applying requirement at least one of them must be b-tagged jet (see details in the specification)\n",
    "    df = (\n",
    "        df.Define(\"Electron_pt_mask\", \"Electron_pt>25\")\n",
    "        .Define(\"Muon_pt_mask\", \"Muon_pt>25\")\n",
    "        .Define(\"Jet_pt_mask\", \"Jet_pt>25\")\n",
    "        .Filter(\"Sum(Electron_pt_mask) + Sum(Muon_pt_mask) == 1\")\n",
    "        .Filter(\"Sum(Jet_pt_mask) >= 4\")\n",
    "    )\n",
    "\n",
    "    # create columns for \"good\" jet pt and btag values as these columns are used several times\n",
    "    df = (\n",
    "        df.Define(\"Jet_pt_ptcut\", \"Jet_pt[Jet_pt_mask]\")\n",
    "          .Define(\"Jet_btagCSVV2_ptcut\", \"Jet_btagCSVV2[Jet_pt_mask]\")\n",
    "    )\n",
    "\n",
    "    # b-tagging variations for nominal samples\n",
    "    if variation == \"nominal\":\n",
    "        df = df.Vary(\n",
    "            \"Weights\",\n",
    "            \"ROOT::RVecD{Weights*btag_weight_variation(Jet_pt_ptcut)}\",\n",
    "            [\n",
    "                f\"{weight_name}_{direction}\"\n",
    "                for weight_name in [f\"btag_var_{i}\" for i in range(4)]\n",
    "                for direction in [\"up\", \"down\"]\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    # Define HT observable for the 4j1b region\n",
    "    # Only one b-tagged region required\n",
    "    # The observable is the total transvesre momentum\n",
    "    # fmt: off\n",
    "    df4j1b = df.Filter(\"Sum(Jet_btagCSVV2_ptcut > 0.5) == 1\")\\\n",
    "               .Define(\"HT\", \"Sum(Jet_pt_ptcut)\")\n",
    "    # fmt: on\n",
    "\n",
    "    # Define trijet_mass observable for the 4j2b region (this one is more complicated)\n",
    "    df4j2b = define_trijet_mass(df)\n",
    "\n",
    "    # Select the right VariationsFor function depending on RDF or DistRDF\n",
    "    if type(df).__module__ == \"DistRDF.Proxy\":\n",
    "        variationsfor_func = ROOT.RDF.Experimental.Distributed.VariationsFor\n",
    "    else:\n",
    "        variationsfor_func = ROOT.RDF.Experimental.VariationsFor\n",
    "\n",
    "    # Book histograms and, if needed, their systematic variations\n",
    "    results = []\n",
    "    for df, observable, region in zip([df4j1b, df4j2b], [\"HT\", \"Trijet_mass\"], [\"4j1b\", \"4j2b\"]):\n",
    "        histo_model = ROOT.RDF.TH1DModel(\n",
    "            name=f\"{region}_{process}_{variation}\", title=process, nbinsx=25, xlow=50, xup=550\n",
    "        )\n",
    "        nominal_histo = df.Histo1D(histo_model, observable, \"Weights\")\n",
    "\n",
    "        if variation == \"nominal\":\n",
    "            varied_histos = variationsfor_func(nominal_histo)\n",
    "            results.append(AGCResult(varied_histos, region, process, variation, nominal_histo))\n",
    "        else:\n",
    "            results.append(AGCResult(nominal_histo, region, process, variation, nominal_histo))\n",
    "        print(f\"Booked histogram {histo_model.fName}\")\n",
    "\n",
    "    # Return the booked results\n",
    "    # Note that no event loop has run yet at this point (RDataFrame is lazy)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a1a1d27-0772-4745-b6d1-421fffe95307",
   "metadata": {},
   "source": [
    "# Execute the analysis and store the output histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736445b0-0183-4362-bb71-4bf1822adfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "program_start = time()\n",
    "\n",
    "# Do not add histograms to TDirectories automatically: we'll do it ourselves as needed.\n",
    "ROOT.TH1.AddDirectory(False)\n",
    "# Disable interactive graphics: avoids canvases flashing on screen before we save them to file\n",
    "ROOT.gROOT.SetBatch(True)\n",
    "\n",
    "if VERBOSE:\n",
    "    # Set higher RDF verbosity for the rest of the program.\n",
    "    # To only change the verbosity in a given scope, use ROOT.Experimental.RLogScopedVerbosity.\n",
    "    ROOT.Detail.RDF.RDFLogChannel.SetVerbosity(ROOT.Experimental.ELogLevel.kInfo)\n",
    "\n",
    "if SCHEDULER == \"mt\":\n",
    "    # Setup for local, multi-thread RDataFrame\n",
    "    ROOT.EnableImplicitMT(NCORES)\n",
    "    print(f\"Number of threads: {ROOT.GetThreadPoolSize()}\")\n",
    "    client = None\n",
    "    load_cpp()\n",
    "    run_graphs = ROOT.RDF.RunGraphs\n",
    "else:\n",
    "    # Setup for distributed RDataFrame\n",
    "    client = create_dask_client(SCHEDULER, NCORES, HOSTS)\n",
    "    ROOT.RDF.Experimental.Distributed.initialize(load_cpp)\n",
    "    run_graphs = ROOT.RDF.Experimental.Distributed.RunGraphs\n",
    "\n",
    "# Book RDataFrame results\n",
    "inputs: list[AGCInput] = retrieve_inputs(\n",
    "    N_MAX_FILES_PER_SAMPLE, REMOTE_DATA_PREFIX, DATA_CACHE\n",
    ")\n",
    "results: list[AGCResult] = []\n",
    "for input in inputs:\n",
    "    df = make_rdf(input.paths, client, NPARTITIONS)\n",
    "    results += book_histos(df, input.process, input.variation, input.nevents)\n",
    "print(f\"Building the computation graphs took {time() - program_start:.2f} seconds\")\n",
    "\n",
    "# Run the event loops for all processes and variations here\n",
    "run_graphs_start = time()\n",
    "run_graphs([r.nominal_histo for r in results])\n",
    "print(f\"Executing the computation graphs took {time() - run_graphs_start:.2f} seconds\")\n",
    "if client is not None:\n",
    "    client.close()\n",
    "\n",
    "results = postprocess_results(results)\n",
    "save_histos([r.histo for r in results], output_fname=OUTPUT)\n",
    "print(f\"Result histograms saved in file {OUTPUT}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e5e9be9-cd46-4d51-956f-2bd7f25d65e8",
   "metadata": {},
   "source": [
    "# Visualize the histograms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19bd6103-e277-409f-be44-5d1cd9fd3d8f",
   "metadata": {},
   "source": [
    "## Create the plotting canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a026311-7b22-4b11-b0be-e22809d58b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 2160\n",
    "height = 2160\n",
    "c = ROOT.TCanvas(\"c\", \"c\", width, height)\n",
    "ROOT.gStyle.SetPalette(ROOT.kRainBow)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49ceddeb-849c-4969-99a3-2daa3dbc3325",
   "metadata": {},
   "source": [
    "## Region 1 stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b9a8b-3639-46ed-ae4b-a71d9f775c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlist = [r.histo for r in results if r.region == \"4j1b\" and r.variation == \"nominal\"]\n",
    "hlist = [h.Clone().Rebin(2) for h in hlist]\n",
    "hs = ROOT.THStack(\"j4b1\", \">=4 jets, 1 b-tag; H_{T} [GeV]\")\n",
    "for h in hlist:\n",
    "    hs.Add(h)\n",
    "hs.Draw(\"hist pfc plc\")\n",
    "c.Draw()\n",
    "x = hs.GetXaxis()\n",
    "x.SetRangeUser(120, x.GetXmax())\n",
    "x.SetTitleOffset(1.5)\n",
    "x.CenterTitle()\n",
    "c.BuildLegend(0.65, 0.7, 0.9, 0.9)\n",
    "c.SaveAs(\"reg1.png\")\n",
    "c.Draw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "533789eb-3d20-432e-a48f-fd1c8498370c",
   "metadata": {},
   "source": [
    "## Region 2 stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291f1e1-12e5-47af-8972-22e07471de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlist = [r.histo for r in results if r.region == \"4j2b\" and r.variation == \"nominal\"]\n",
    "hs = ROOT.THStack(\"j4b1\", \">=4 jets, 2 b-tag; H_{T} [GeV]\")\n",
    "for h in hlist:\n",
    "    hs.Add(h)\n",
    "hs.Draw(\"hist pfc plc\")\n",
    "c.Draw()\n",
    "x = hs.GetXaxis()\n",
    "x.SetTitleOffset(1.5)\n",
    "x.CenterTitle()\n",
    "c.BuildLegend(0.65, 0.7, 0.9, 0.9)\n",
    "c.SaveAs(\"reg2.png\")\n",
    "c.Draw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffd12eb5-2dd1-417c-a7a0-5ce17b381b11",
   "metadata": {},
   "source": [
    "## b-tag variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2939410-8937-4372-be65-b18f94c6f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "btag_variations = [\n",
    "    \"nominal\",\n",
    "    \"btag_var_0_up\",\n",
    "    \"btag_var_1_up\",\n",
    "    \"btag_var_2_up\",\n",
    "    \"btag_var_3_up\",\n",
    "]\n",
    "\n",
    "\n",
    "def btag_filter(r):\n",
    "    return r.region == \"4j1b\" and r.process == \"ttbar\" and r.variation in btag_variations\n",
    "\n",
    "\n",
    "hlist = [r.histo for r in results if btag_filter(r)]\n",
    "hlist = [h.Clone().Rebin(2) for h in hlist]\n",
    "hs = ROOT.THStack(\"j4b1btag\", \"btag-variations ; H_{T} [GeV]\")\n",
    "for h, name in zip(hlist, btag_variations):\n",
    "    h.SetLineWidth(4)\n",
    "    h.SetTitle(name)\n",
    "    hs.Add(h)\n",
    "hs.Draw(\"hist nostack plc\")\n",
    "c.Draw()\n",
    "x = hs.GetXaxis()\n",
    "x.SetRangeUser(120, x.GetXmax())\n",
    "x.SetTitleOffset(1.5)\n",
    "x.CenterTitle()\n",
    "c.BuildLegend(0.65, 0.7, 0.9, 0.9)\n",
    "c.SaveAs(\"btag.png\")\n",
    "c.Draw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a1b63fe-3842-4a62-bdc6-4b9cc28059e5",
   "metadata": {},
   "source": [
    "## Jet energy variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2187a-c6bd-48de-8232-42d5436f0659",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_variations = [\"nominal\", \"pt_scale_up\", \"pt_res_up\"]\n",
    "\n",
    "\n",
    "def jet_filter(r):\n",
    "    return r.region == \"4j2b\" and r.process == \"ttbar\" and r.variation in jet_variations\n",
    "\n",
    "\n",
    "hlist = [r.histo for r in results if jet_filter(r)]\n",
    "hs = ROOT.THStack(\"4j2bjet\", \"Jet energy variations ; m_{bjj} [GeV]\")\n",
    "for h, name in zip(hlist, jet_variations):\n",
    "    h.SetFillColor(0)\n",
    "    h.SetLineWidth(4)\n",
    "    h.SetTitle(name)\n",
    "    hs.Add(h)\n",
    "hs.Draw(\"hist nostack plc\")\n",
    "c.Draw()\n",
    "x = hs.GetXaxis()\n",
    "x.SetRangeUser(0, 600)\n",
    "x.SetTitleOffset(1.5)\n",
    "x.CenterTitle()\n",
    "c.BuildLegend(0.65, 0.7, 0.9, 0.9)\n",
    "c.SaveAs(\"jet.png\")\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b79d2ac-3716-431c-9ded-68d5f146f76e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
